{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0aacab-df26-4d95-b7b3-f190e7143592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning model to predict whether financial statements indicate a <6% Tier 1 Capital Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9344f-ff44-450b-8f46-27d78ba9411a",
   "metadata": {},
   "source": [
    "1. See what columns are essential\n",
    "    * Load in first two columns of every file\n",
    "    * def load_one(path):\n",
    "    Read the file (tab-delimited, skip the description row)\n",
    "\n",
    "    df = pd.read_csv(\n",
    "   \n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "        nrows=2,\n",
    "        dtype=str\n",
    "    )\n",
    "    * Look at descriptions and how many files have what\n",
    "3. Determine essential columns to keep and notate\n",
    "4. Any files that don't have essential columns are dropped\n",
    "5. Read in all files, merge to single DataFrame\n",
    "6. Force essential columns to float32\n",
    "    * df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(np.float32) for c in essential_cols\n",
    "    * Some models force conversion to float64, sklearn LogisticRegression does this\n",
    "7. Train, Test, Split\n",
    "8. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9a580-b827-4c71-b7fc-2015d8b1ba90",
   "metadata": {},
   "source": [
    "FFIEC Website: https://cdr.ffiec.gov/public/ManageFacsimiles.aspx\n",
    "\n",
    "Bulk Data: https://cdr.ffiec.gov/public/PWS/DownloadBulkData.aspx\n",
    "\n",
    "Silent trip ups: \n",
    "\n",
    "- Have non-proportional amounts of what you're classifying.\n",
    "\n",
    "- Not having enought samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7338a88-e840-46b2-8d2a-060b5b07c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "# 1. Function to import clean dataset | DONE\n",
    "# 2. Create list of dataframes for each file\n",
    "# 3. pd.concat for each in list\n",
    "def clean_import(x):\n",
    "        df_head = pd.read_csv(\n",
    "            x,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            nrows=2,\n",
    "            engine=\"python\",\n",
    "            dtype=str\n",
    "        )\n",
    "        # Fill NaNs with empty string\n",
    "        df_head = df_head.fillna(\"\")\n",
    "        # Drop columns where both code and description are empty\n",
    "        keep = ~((df_head.iloc[0] == \"\") & (df_head.iloc[1] == \"\"))\n",
    "        df_head = df_head.loc[:, keep]\n",
    "        return df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0114d0d9-c17e-477f-8611-13d58d6f3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "# 1. Function to import clean dataset | DONE\n",
    "# 2. Create list of dataframes for each file\n",
    "# 3. pd.concat for each in list\n",
    "def clean_import(x):\n",
    "        df_head = pd.read_csv(\n",
    "            x,\n",
    "            sep=\"\\t\",\n",
    "            header=0,\n",
    "            engine=\"python\",\n",
    "            dtype=str\n",
    "        )\n",
    "        # Fill NaNs with empty string\n",
    "        df_head = df_head.fillna(\"\")\n",
    "        # Drop columns where both code and description are empty\n",
    "        keep = ~((df_head.iloc[0] == \"\") & (df_head.iloc[1] == \"\"))\n",
    "        df_head = df_head.loc[:, keep]\n",
    "        return df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e24f58-698c-47ae-9cf7-18fd4dab4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: practice loopping through folders, look some examples up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce0d4e10-3ea8-4eaa-aeca-287cbea351d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'WindowsPath' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m input_dir = Path(\u001b[33m'\u001b[39m\u001b[33mC:/Users/Mark/Desktop/Python/github/ml-regulatory-project/RCRI Schedules\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m files = \u001b[38;5;28mlist\u001b[39m(input_dir)\n\u001b[32m      3\u001b[39m files\n",
      "\u001b[31mTypeError\u001b[39m: 'WindowsPath' object is not iterable"
     ]
    }
   ],
   "source": [
    "input_dir = Path('C:/Users/Mark/Desktop/Python/github/ml-regulatory-project/RCRI Schedules')\n",
    "files = list(input_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3361ad8e-9036-4bd4-be90-81e8e24aa701",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Combine all into one big DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m panel = pd.concat(dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m    385\u001b[39m     ignore_index=ignore_index,\n\u001b[32m    386\u001b[39m     join=join,\n\u001b[32m    387\u001b[39m     keys=keys,\n\u001b[32m    388\u001b[39m     levels=levels,\n\u001b[32m    389\u001b[39m     names=names,\n\u001b[32m    390\u001b[39m     verify_integrity=verify_integrity,\n\u001b[32m    391\u001b[39m     copy=copy,\n\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28mself\u001b[39m._clean_keys_and_objs(objs, keys)\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\machine_learning\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "folder = Path(r'\\ml-regulatory-project\\RCRI Schedules')\n",
    "\n",
    "for path in sorted(folder.glob(\"*.txt\")):\n",
    "    try:\n",
    "        df = clean_import(path)\n",
    "        df[\"source_file\"] = path.name   # track origin\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {path.name}: {e}\")\n",
    "\n",
    "# Combine all into one big DataFrame\n",
    "panel = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e378d53-44b1-4c7f-81fc-3adda07a93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9115437-3a0f-495e-b60a-97e6395d0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c74d1d-245e-463d-9ddf-3847bdfa1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of columns with zero data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af5b12-8773-4ced-a23c-746b6d1f068f",
   "metadata": {},
   "source": [
    "F1-Score - a machine learning evaluation metric that measures a classifier's accuracy by taking the harmonic mean of precision and recall. It's a particularly useful metric for evaluating models, especially with imbalanced datasets, because it balances the trade-off between high precision and high recall. A perfect F1 score is 1 (100%), while the worst score is 0.\n",
    "https://www.google.com/search?client=firefox-b-1-d&q=machine+learning+f1+score\n",
    "\n",
    "Mean Absolute Error - Average distance of all data points from fitted trend line\n",
    "\n",
    "Validation Accuracy (For Random Forest) - How often model is correct overall. Not as useful if you don't have 50/50 split of classes (or 25/25/25/25 or whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3fc9f-fefb-4d9b-a2d1-25c280eeeb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one file. \n",
    "# List columns and descriptions\n",
    "# List any file that does not contain that exact same columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99cb110-3a63-451c-a9e1-5f73e3c812cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need more quarters worth of data, get at least 50 < 6%s. Just download as many as possible and look at\n",
    "# Look at all columns for each file\n",
    "# Any that are exactly the same, great\n",
    "# If not, do they have what we need?\n",
    "# Check dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
