{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0aacab-df26-4d95-b7b3-f190e7143592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning model to predict whether financial statements indicate a <6% Tier 1 Capital Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9344f-ff44-450b-8f46-27d78ba9411a",
   "metadata": {},
   "source": [
    "1. See what columns are essential\n",
    "    * Load in first two columns of every file\n",
    "    * def load_one(path):\n",
    "    Read the file (tab-delimited, skip the description row)\n",
    "\n",
    "    df = pd.read_csv(\n",
    "   \n",
    "        path,\n",
    "        sep=\"\\t\",\n",
    "        header=0,\n",
    "        nrows=2,\n",
    "        dtype=str\n",
    "    )\n",
    "    * Look at descriptions and how many files have what\n",
    "3. Determine essential columns to keep and notate\n",
    "4. Any files that don't have essential columns are dropped\n",
    "5. Read in all files, merge to single DataFrame\n",
    "6. Force essential columns to float32\n",
    "    * df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(np.float32) for c in essential_cols\n",
    "    * Some models force conversion to float64, sklearn LogisticRegression does this\n",
    "7. Train, Test, Split\n",
    "8. Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f9a580-b827-4c71-b7fc-2015d8b1ba90",
   "metadata": {},
   "source": [
    "FFIEC Website: https://cdr.ffiec.gov/public/ManageFacsimiles.aspx\n",
    "\n",
    "Bulk Data: https://cdr.ffiec.gov/public/PWS/DownloadBulkData.aspx\n",
    "\n",
    "Silent trip ups: \n",
    "\n",
    "- Have non-proportional amounts of what you're classifying.\n",
    "\n",
    "- Not having enought samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7338a88-e840-46b2-8d2a-060b5b07c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "# 1. Function to import clean dataset | DONE\n",
    "# 2. Create list of dataframes for each file\n",
    "# 3. pd.concat for each in list\n",
    "def clean_import(x):\n",
    "        df_head = pd.read_csv(\n",
    "            x,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            nrows=2,\n",
    "            engine=\"python\",\n",
    "            dtype=str\n",
    "        )\n",
    "        # Fill NaNs with empty string\n",
    "        df_head = df_head.fillna(\"\")\n",
    "        # Drop columns where both code and description are empty\n",
    "        keep = ~((df_head.iloc[0] == \"\") & (df_head.iloc[1] == \"\"))\n",
    "        df_head = df_head.loc[:, keep]\n",
    "        return df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0114d0d9-c17e-477f-8611-13d58d6f3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "# 1. Function to import clean dataset | DONE\n",
    "# 2. Create list of dataframes for each file\n",
    "# 3. pd.concat for each in list\n",
    "def clean_import(x):\n",
    "        df_head = pd.read_csv(\n",
    "            x,\n",
    "            sep=\"\\t\",\n",
    "            header=0,\n",
    "            engine=\"python\",\n",
    "            dtype=str\n",
    "        )\n",
    "        # Fill NaNs with empty string\n",
    "        df_head = df_head.fillna(\"\")\n",
    "        # Drop columns where both code and description are empty\n",
    "        keep = ~((df_head.iloc[0] == \"\") & (df_head.iloc[1] == \"\"))\n",
    "        df_head = df_head.loc[:, keep]\n",
    "        return df_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e24f58-698c-47ae-9cf7-18fd4dab4f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Have folder refer to our RCRI Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3361ad8e-9036-4bd4-be90-81e8e24aa701",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'glob'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m dfs = []\n\u001b[32m      2\u001b[39m folder = glob.glob(\u001b[33m'\u001b[39m\u001b[33m/ml-regulatory-project\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mRCRI Schedules\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(folder.glob(\u001b[33m\"\u001b[39m\u001b[33m*.txt\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      6\u001b[39m         df = clean_import(path)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'glob'"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "folder = glob.glob('/ml-regulatory-project\\RCRI Schedules') # This does not work\n",
    "\n",
    "for path in sorted(folder.glob(\"*.txt\")):\n",
    "    try:\n",
    "        df = clean_import(path)\n",
    "        df[\"source_file\"] = path.name   # track origin\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {path.name}: {e}\")\n",
    "\n",
    "# Combine all into one big DataFrame\n",
    "panel = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e378d53-44b1-4c7f-81fc-3adda07a93fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9115437-3a0f-495e-b60a-97e6395d0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c74d1d-245e-463d-9ddf-3847bdfa1920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of columns with zero data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af5b12-8773-4ced-a23c-746b6d1f068f",
   "metadata": {},
   "source": [
    "F1-Score - a machine learning evaluation metric that measures a classifier's accuracy by taking the harmonic mean of precision and recall. It's a particularly useful metric for evaluating models, especially with imbalanced datasets, because it balances the trade-off between high precision and high recall. A perfect F1 score is 1 (100%), while the worst score is 0.\n",
    "https://www.google.com/search?client=firefox-b-1-d&q=machine+learning+f1+score\n",
    "\n",
    "Mean Absolute Error - Average distance of all data points from fitted trend line\n",
    "\n",
    "Validation Accuracy (For Random Forest) - How often model is correct overall. Not as useful if you don't have 50/50 split of classes (or 25/25/25/25 or whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c3fc9f-fefb-4d9b-a2d1-25c280eeeb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in one file. \n",
    "# List columns and descriptions\n",
    "# List any file that does not contain that exact same columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99cb110-3a63-451c-a9e1-5f73e3c812cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need more quarters worth of data, get at least 50 < 6%s. Just download as many as possible and look at\n",
    "# Look at all columns for each file\n",
    "# Any that are exactly the same, great\n",
    "# If not, do they have what we need?\n",
    "# Check dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
